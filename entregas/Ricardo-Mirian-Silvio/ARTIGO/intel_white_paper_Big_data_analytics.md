Extrair, transformar e carregar grandes volumes de dados com Apache Hadoop*


ABSTRACT

Nos últimos anos, organizações de todos os setores público e privado tomaram a decisão estratégica de transformar grandes dados em vantagem competitiva. O desafio de extrair valor de soluções big data  é similar em muitos aspectos ao antigo problema de obter inteligência de negócios a partir de dados transacionais. No centro deste desafio está o processo usado para extrair dados de múltiplas fontes, transformá-lo para atender às suas necessidades analíticas e carregá-lo em um data warehouse para posterior análise, um processo conhecido como "Extract, Transform & Load" (ETL) . A natureza big data exige que a infra-estrutura para esse processo possa ser escalada de forma econômica. Apache Hadoop  surgiu como o padrão de fato para a gestão de big data. Este white paper examina algumas das considerações de hardware e software da plataforma ao usar o Hadoop para ETL.
- Planejamos publicar outros white papers que mostram como uma plataforma baseada no Apache Hadoop pode ser estendida para suportar consultas interativas e análises preditivas em tempo real. Quando finalizados, esses white papers estarão disponíveis em http://hadoop.intel.com.

O Gargalo de ETL em Big Data Analytics
Big Data refere-se a grandes quantidades, pelo menos terabytes de dados poli-estruturados que transitam continuamente através e em torno de organizações, incluindo vídeo, texto, logs de sensores e registros transacionais. Os benefícios do negócio de analisar esses dados podem ser significativos. De acordo com um estudo recente do MIT Sloan School of Management, as organizações que utilizam a analytics têm o dobro da probabilidade de serem top performers em sua indústria em relação as que não utilizam.
Analistas de negócios em uma grande empresa como a Intel, por exemplo, com seu mercado global e cadeia de suprimentos complexa, há muito tempo buscam informações sobre a demanda dos clientes, analisando um grande volume de dados retirados de informações de mercado e transações comerciais. Cada vez mais, os dados que precisamos estão incorporados em relatórios econômicos, fóruns de discussão, sites de notícias, redes sociais, relatórios de tempo, wikis, tweets e blogs, bem como transações. Analisando todos os dados disponíveis, os tomadores de decisão podem avaliar melhor as ameaças competitivas, antecipar mudanças no comportamento dos clientes, fortalecer as cadeias de suprimentos, melhorar a eficácia das campanhas de marketing e melhorar a continuidade dos negócios.
Muitos desses benefícios não são novos para organizações que têm processos maduros para incorporar inteligência de negócios (BI) e análise em suas tomadas de decisão. No entanto, a maioria das organizações ainda tem que tirar o máximo de vantagem das novas tecnologias para lidar com big data. Simplificando, o custo das tecnologias necessárias para armazenar e analisar grandes volumes de dados diversos caiu, graças a softwares de código aberto sendo executados em hardwares padronizados da indústria. O custo caiu tanto, na verdade, que a questão chave da estratégia não é mais o quais dados são relevantes, mas sim como extrair o máximo valor de todos os dados disponíveis.
A rápida importação, armazenamento e processamento de big data requer uma infraestrutura de baixo custo que pode ser escalonada com a quantidade de dados e o escopo da análise. A maioria das organizações com plataformas de dados tradicionais com sistemas de gerenciamento de bancos de dados relacionais (RDBMS) conectados a data warehouses corporativos (EDW) que utilizam ferramentas ETL, descobrem que sua infraestrutura legada é tecnicamente incapaz ou financeiramente inviável para o armazenamento e analise de grandes volumes de dados.
Um processo ETL tradicional extrai dados de várias fontes, depois limpa, formata e carrega em um data warehouse para análise. Quando os conjuntos de dados são grandes, rápidos e não estruturados, o ETL tradicional pode se tornar o gargalo, porque é muito complexo de se desenvolver, muito caro de operar e demoraria muito para ser executado.
 
Na maioria dos casos, 80 por cento do esforço de desenvolvimento em um grande projeto de dados vai para a integração de dados e apenas 20 por cento vai para análise de dados. Além disso, uma plataforma tradicional EDW pode custar mais de USD 60K por terabyte. Analisando um petabyte, a quantidade de dados que o Google processa 1 hora custaria USD 60M. Claramente não é uma estratégia para grandes volumes de dados que qualquer CIO pode pagar. Neste caso, entra o Apache Hadoop.

Apache Hadoop para Big Data
Quando o Yahoo, o Google, o Facebook e outras empresas estenderam seus serviços à escala de internet, a quantidade de dados coletados rotineiramente das interações do usuário on-line já teria superado os recursos das arquiteturas de TI tradicionais. Então eles se reconstruíram. Com o interesse de promover o desenvolvimento de componentes de infraestrutura básica rapidamente, eles publicaram artigos e liberaram código para muitos dos componentes em código aberto. Destes componentes, o Apache Hadoop emergiu rapidamente como o padrão de fato para o gerenciamento de grandes volumes de dados não estruturados.
Apache Hadoop é uma plataforma de software distribuído de código aberto para armazenar e processar dados. Escrito em Java, ele é executado em um cluster de servidores padrão da indústria, configurados com conexão direta de armazenamento. Usando o Hadoop, você pode armazenar petabytes de dados de forma confiável em dezenas de milhares de servidores ao dimensionar o desempenho de forma econômica simplesmente adicionando nós de baixo custo ao cluster.
A central para a escalabilidade do Apache Hadoop é o quadro de processamento distribuído conhecido como MapReduce (Figura 1). O MapReduce ajuda os programadores a resolver problemas de dados paralelos para os quais o conjunto de dados pode ser subdividido em pequenas partes e processado de forma independente. MapReduce é um avanço importante porque permite que desenvolvedores comuns, não apenas aqueles qualificados em computação de alto desempenho, usem construções de programação paralela sem se preocupar com os detalhes complexos de comunicação intra-cluster, monitoramento de tarefas e manipulação de falhas. MapReduce simplifica tudo isso.
O sistema divide o conjunto de dados de entrada em vários blocos, cada um dos quais é atribuído uma tarefa de map que pode processar os dados em paralelo. Cada tarefa de map lê a entrada como um conjunto de pares (chave, valor) e produz um conjunto transformado de pares (chave, valor) como a saída. A estrutura embaralha e classifica as saídas das tarefas de map, enviando os pares intermediários (chave, valor) para as tarefas de redução, que os agrupam em resultados finais. O MapReduce usa os mecanismos JobTracker e TaskTracker para agendar tarefas, monitorá-las e reiniciar a qualquer falha.
A plataforma Apache Hadoop também inclui o Sistema de Arquivos Distribuídos Hadoop [Hadoop Distributed File System] (HDFS), projetado para escalabilidade e tolerância a falhas. O HDFS armazena grandes arquivos dividindo-os em blocos (geralmente 64 ou 128 MB) e replicando os blocos em três ou mais servidores. O HDFS fornece APIs para aplicações MapReduce para ler e gravar dados em paralelo. A capacidade e o desempenho podem ser dimensionados adicionando Data Nodes, e um único mecanismo de NameNode gerencia o posicionamento de dados e monitora a disponibilidade do servidor. Os clusters HDFS em produção utilizam hoje, de forma confiável, petabytes de dados em milhares de nós.

 


Figura 1. MapReduce, o paradigma de programação implementado pelo Apache Hadoop, divide um trabalho em lote em muitas tarefas menores para processamento paralelo em um sistema distribuído. HDFS, o sistema de arquivos distribuídos armazena os dados de forma confiável.

Além de MapReduce e HDFS, o Apache Hadoop inclui muitos outros componentes, alguns dos quais são muito úteis para ETL.

• Apache Flume* é um sistema distribuído para coletar, agregar e mover grandes quantidades de dados de várias fontes para o HDFS ou outra central de armazenamento de dados. As empresas geralmente coletam arquivos de log em servidores de aplicativos ou outros sistemas e arquivam os arquivos de log para cumprir com os regulamentos. Ser capaz de assimilar e analisar os dados não estruturados ou semi-estruturados no Hadoop podem transformar este recurso passivo em um ativo valioso.

• Apache Sqoop* é uma ferramenta para transferir dados entre Hadoop e bancos de dados relacionais. Você pode usar o Sqoop para importar dados de um banco de dados MySQL ou Oracle para HDFS, executar MapReduce nos dados e em seguida, exportar os dados de volta para um RDBMS. O Sqoop automatiza esses processos, usando MapReduce para importar e exportar os dados em paralelo com tolerância a falhas.

• Apache Hive* e Apache Pig* são linguagens de programação que simplificam o desenvolvimento de aplicações que utilizam o framework MapReduce. HiveQL é um dialeto de SQL e suporta um subconjunto da sintaxe. Embora lento, o Hive está sendo ativamente aprimorado pela comunidade de desenvolvedores para permitir consultas de baixa latência no Apache HBase* e HDFS. Pig Latin é uma linguagem de programação procedural que fornece abstrações de alto nível para MapReduce. Você pode estendê-lo com funções definidas pelo usuário escritas em Java, Python e outros idiomas.

• Conectores ODBC/JDBC para HBase e Hive são componentes proprietários incluídos nas distribuições do software Apache Hadoop. Eles fornecem conectividade com aplicativos SQL, traduzindo consultas SQL padrão em comandos HiveQL que podem ser executados sobre os dados em HDFS ou HBase.

O Hadoop é uma poderosa plataforma para armazenamento e processamento de grandes volumes de dados. No entanto, sua extensibilidade e novidade geram questões em torno da integração de dados, da qualidade dos dados, da governança, da segurança e de uma série de outros problemas que as empresas com processos maduros de BI têm como certo. Apesar dos muitos desafios da integração do Hadoop em um ambiente tradicional de BI, o ETL provou ser um caso de uso frequente para o Hadoop nas empresas. Então o que explica a sua popularidade?


ETL, ELT e ETLT com Apache Hadoop

As ferramentas ETL movem dados de um lugar para outro executando três funções:

• Extração de dados de fontes como aplicações ERP ou CRM. 
Durante a etapa de extração, talvez seja necessário coletar dados de vários sistemas de origem e em vários formatos de arquivo, como arquivos com delimitadores (CSV) e arquivos XML. Você também pode precisar coletar dados de sistemas legados que armazenam dados em formatos mais antigos que ninguém mais usa. Isso parece fácil, mas pode de fato ser um dos principais obstáculos para obter uma solução ETL.

• Transformar esses dados em um formato comum que se ajuste aos outros dados no warehouse. 
O passo de transformação pode incluir múltiplas manipulações de dados, tais como movimentação, divisão, tradução, fusão, ordenação, rotação e muito mais. Por exemplo, um nome de cliente pode ser dividido em nome e sobrenome, ou datas podem ser alteradas para o formato ISO padrão (por exemplo, de 07-24-13 a 2013-07-24). Muitas vezes, esta etapa também envolve a validação dos dados contra as regras de qualidade dos dados.

• Carregar os dados no data warehouse para análise. 
Esta etapa pode ser feita por processos em lote ou linha a linha, mais ou menos em tempo real.

No começo, antes das ferramentas ETL existirem, a única maneira de integrar dados de fontes diferentes era manusear scripts em linguagens como COBOL, RPG e PL/SQL. Embora pareça antiquado, cerca de 45 por cento de todo o trabalho ETL hoje ainda é feito por estes programas codificados manualmente. Mesmo que eles sejam propensos a erros, lentos para serem desenvolvidos e difíceis de se manter, eles têm usuários leais que parecem impermeáveis aos encantos das ferramentas ETL, como o Oracle Warehouse Builder*, que pode gerar código para bancos de dados.

Os geradores de código também têm limitações, já que eles trabalham com apenas um conjunto limitado de bancos de dados e geralmente são empacotados com eles. Em contraste, a próxima geração de ferramentas ETL inclui um mecanismo de propósito mais amplo que executa a transformação em um conjunto de metadados e armazena a lógica de transformação. Como as ferramentas ETL baseadas em motores, como Pentaho Kettle * e Informatica Powercenter *, que são independentes dos armazenamentos de dados de origem e de destino, são mais versáteis do que as ferramentas de geração de código.

Uma arquitetura ETL tradicional (Figura 2) acomoda múltiplas iterações ETL, bem como uma etapa intermediária, realizada na "área de teste", que obtém os dados dos sistemas de origem o mais rápido possível. Uma área de teste pode usar um banco de dados ou simplesmente arquivos CSV, o que torna o processo mais rápido do que inserir dados em uma tabela de banco de dados. Podem ser implementadas iterações ETL adicionais para transferir dados do EDW para data marts, que suportam fins analíticos específicos e ferramentas de usuário final.

Muito mudou em data warehousing ao longo das duas últimas décadas. Principalmente, as bases de dados tornaram-se muito mais poderosas. Os motores RDBMS agora suportam transformações complexas em SQL, incluindo mineração de dados no banco de dados, validação de qualidade de dados de banco de dados, limpeza, criação de perfis, algoritmos estatísticos, funcionalidades hierárquicas, drill-down e muito mais. Tornou-se mais eficiente executar a maioria dos tipos de "transformação" dentro do mecanismo RDBMS.

 
Figura 2. A tradicional arquitetura ETL tem servido bem as empresas nos últimos 20 anos e muitas implantações ainda a usam.

Como resultado, o ELT surgiu como uma abordagem alternativa na qual os dados são extraídos das fontes, carregados no banco de dados de destino e depois transformados e integrados no formato desejado. Todo o processamento de dados pesados ocorre dentro do banco de dados de destino. A vantagem desta abordagem é que um sistema de banco de dados é mais adequado para lidar com grandes cargas de trabalho para as quais centenas de milhões de registros precisam ser integrados. Os motores RDBMS também são otimizados para os discos de E/S, o que aumenta a taxa de transferência. E, enquanto o hardware RDBMS aumentar, o desempenho do sistema será escalado com ele.

Mas a ETL ainda não morreu. Os fornecedores ETL tradicionais melhoraram suas ferramentas com capacidades SQL pushdown nas quais a transformação pode ocorrer no mecanismo ETL (para operações não suportadas pelo banco de dados de destino) ou após o carregamento dentro do banco de dados. O resultado é a abordagem ETLT, suportada igualmente bem pelos principais fornecedores de bancos de dados, como Microsoft (SQL Server Integration Services) e Oracle (Oracle Warehouse Builder).

Nenhuma dessas soluções é barata ou simples, e seu custo e complexidade são agravados com grandes volumes de dados. Considere o eBay, que em 2011 tinha mais de 200 milhões de itens para venda, separados em 50.000 categorias, e comprados e vendidos por 100 milhões de usuários registrados, o que envolveu cerca de 9 petabytes de dados. O Google supostamente processa mais de 24 petabytes de dados por dia. A AT & T processa 19 petabytes através de suas redes a cada dia e o jogo World of Warcraft usa 1.3 petabytes de armazenamento. Todos esses números já estão desatualizados até agora, pois os dados on-line estão crescendo muito rápido.

Nestas circunstâncias, o Hadoop traz pelo menos duas grandes vantagens ao ETLT tradicional:

• Administrar quantidades maciças de dados sem especificar um esquema na gravação. 
Uma característica chave do Hadoop é chamada de "não especificação de gravação", o que significa que você não precisa predefinir o esquema de dados antes de carregar dados no Hadoop. Isso é verdade não apenas para dados estruturados (como transações de ponto de venda, registros de detalhes de chamadas, transações de razão geral e operações de call center), mas também para dados não estruturados (como comentários de usuários, notas médicas, e web logs) e dados de mídia social (de sites como Facebook, LinkedIn, Pinterest e Twitter). Independentemente de seus dados recebidos terem estrutura explícita ou implícita, você pode rapidamente carregá-los como estão no Hadoop, onde estarão disponíveis para refinamento em processos analíticos.

• Importando a transformação de dados brutos por processamento paralelo em escala. 
Uma vez que os dados estejam no Hadoop (ou um sistema de arquivos compatível com Hadoop), você pode executar as tarefas ETL tradicionais de limpeza, normalização, alinhamento e agregação de dados para o EDW, empregando a escalabilidade maciça do MapReduce. O Hadoop permite evitar o gargalo de transformação em seu ETLT tradicional transportando a importação, transformação e integração de dados não estruturados em seu data warehouse (Figura 3). Como o Hadoop permite que você utilize mais tipos de dados do normalmente você conseguiria, ele enriquece o seu data warehouse de maneiras que de outra forma seriam inviáveis ou proibitivas. Devido ao seu desempenho escalável, você pode acelerar significativamente os trabalhos ETLT. Além disso, como os dados armazenados no Hadoop podem persistir durante muito mais tempo, você pode fornecer dados mais detalhados e proporcionar através de seu EDW análises de mais fiéis.

 

Figura 3. Usando o Apache Hadoop, as organizações podem importar, processar e exportar quantidades maciças de diversos dados em escala.

Usando Hadoop desta forma, a organização ganha uma capacidade adicional para armazenar e acessar dados que eles "podem" precisar, dados que poderão nunca ser carregados no data warehouse. Por exemplo, os cientistas de dados podem querer usar grandes quantidades de dados de origem de mídia social, web logs ou lojas de terceiros (de curadores, tais como data.gov) armazenados em modelos analíticos Hadoop para melhorar uma nova unidade de pesquisa e desenvolvimento. Eles podem armazenar dados com melhor custo benefício no Hadoop, e recuperá-lo conforme necessário (usando outras ferramentas analíticas como Hive ou sistemas nativos para a plataforma), sem afetar o ambiente EDW.

Independentemente de saber se a sua empresa é de ETL, ELT, ou abordagem ETLT para armazenamento de dados, você pode reduzir o custo operacional de sua solução global de BI/DW, transferindo pipelines de transformação comuns a Apache Hadoop e usando MapReduce no HDFS para fornecer uma solução escalável, tolerantes a falha de plataforma para processar grandes quantidades de dados heterogéneos.


Escolhendo a Infraestrutura Física para ETL Com Hadoop
A regra de ouro para o planejamento da infraestrutura Hadoop tem sido "incluir mais nós para resolver o problema." Esta é uma abordagem razoável quando o tamanho do seu cluster permite desafio de aumentar em escala da Web, como retornar resultados de pesquisa ou personalizar páginas da web para centenas de milhões de compradores online.  Mas um cluster Hadoop típico em uma empresa tem cerca de 100 nós e é apoiado por recursos de TI que são significativamente mais limitados do que Yahoo! e Facebook.  Organizações devem aderir a gestão destes clusters para o planejamento de capacidade e ajuste de desempenho típico de outros Processos de infraestruturas de TI. Mais cedo ou mais tarde, eles precisarão prover, configurar e se ajustar com o Hadoop suas cargas de trabalho específicas e execução dos seus projetos.

Os workloads variam muito, por isso é importante selecionar e configurar computação, armazenamento, rede e infraestrutura de software para atender às necessidades específicas. Mas antes de considerar cada uma dessas questões, vamos examinar o equívoco mais comum de todas as cargas de trabalho do Hadoop são I/O-bound. Nos últimos 3 anos, desenvolvedores da Intel tem testado o desempenho das sucessivas atualizações do Hadoop nas sucessivas gerações de servidores baseados em processadores Intel usando uma referência de conjunto de workloads de testes. Muitos desses workloads, foram testados em agregados semelhantes a aplicações do mundo real, tais como ETL e Analytics. Baseado na instrumentação de profundidade de teste de clusters, percebemos que tanto de I/O quanto CPU utilizam significativamente vários workloads em estágios diferente do MapReduce;

• TeraSort (Map: CPU Bound;  Reduce: I/O bound) TeraSort transforma os dados de uma representação para outra. Uma vez que o tamanho de dados continua a ser o mesmo a partir da entrada para a saída através de reprodução aleatória, TeraSort tende a ser I/O bound.  No entanto, comprime a saída do Map de modo a minimizar utilização de disco e entrada e saída de rede durante a fase de reprodução aleatória, TeraSort mostra uma utilização muito elevada da CPU e moderada de e/s do disco durante o Map e a fase de reprodução aleatória, e moderada utilização da CPU e elevada utilização de e/s do disco durante o processo de Reduce;

• WordCount (CPU Bound) WordCount extrai uma pequena quantidade de informações interessantes a partir de um grande conjunto de dados, o que significa que a saída do Map e a saída do Reduce são muito menores do que os dados de entrada. Como resultado, no WordCount é maior a utilização da cpu no workload (especialmente durante a etapa de Map), com elevada utilização de CPU e pouca utilização de disco e e/s de rede;

• Nutch Indexing (Map: CPU Bound; Reduce: I/O bound) A entrada para esse workload é de cerca de 2,4 milhões de páginas web geradas por meio da indexação do Wikipedia. Nutch Indexing descompacta os dados na fase de Map, com maior utilização de cpu, e os resultados intermediários convertem os arquivos de índice na fase de Reduce, com elevada utilização de E/S;

• PageRank Search (CPU Bound) O workload do Page Rank é representante da inspiração original para o algoritmo de Google Search baseado no PageRank para link analysis. Este workload passa a maior parte do seu tempo em várias iterações de Jobs, e esses jobs geralmente tem uma elevada utilização de CPU e pouca para média utilização de disco e utilização de memória;

• Bayesian Classification (I/O Bound) Esse workloads implementa o treino do classificador Bayesian Naïve, um algoritmo popular para descoberta de conhecimento e mineração de dados. O workload contém quatro jobs Hadoop vinculados, que se utilizando de E/S de disco, exceto pelo primeiro job utilizado para tarefas de Map, que tem uma elevada utilização de CPU e será mais demorado.

• K-means Clustering (CPU Bound in iteration; I/O Bound in clustering) Este workload primeiro computa o centro de cada cluster executando o job do Hadoop iterativamente convergindo as iterações até que o número máximo de iterações é alcançado. Tem elevada utilização de CPU. Depois disso, ele executa um job clusterizando (E/S elevada) que atribui cada cluster uma amostra;

Mesmo um exame superficial de vários workloads mostram que a utilização de recursos do sistema de Hadoop é mais crítica para as empresas que se tornaram pioneiras e mais complexo do que muitos previram. Intel tem trabalhado em estreita colaboração com um número de clientes para ajudar a desenvolver e implementar uma plataforma equilibrada para implementações reais de Hadoop. Nossas avaliações e esforços de benchmarking nos levou a fazer várias recomendações e considerações a clientes quanto a infraestrutura de hardware e distribuições de software Apache Hadoop.

Computar

O processador Intel® Xeon® da família E5 fornece uma base sólida para muitos workloads do Hadoop. Uma série de características incorporadas ao processador Intel Xeon da família E5 são adequados particularmente para Hadoop. Uma característica é Intel® Integrated I/O, que pode reduzir a latência de E/S em até 32 por cento e incremento de largura de banda de E/S em até 2x.2,3. Outra característica é o Direct I/O Technology (DDIO), que permite que os adaptadores Intel® Ethernet para comunicação direta com cache do processador e não apenas com a memória principal. Esse recurso fornece mais largura de banda de E/S e menor latência, o que é particularmente benéfico quando processa grandes conjuntos de dados. Outra característica que tem atraído a atenção dos usuários do Hadoop é Intel® Advanced Encryption Standard New Instructions (AES-NI), que acelera funções criptográficas comuns para diminuir a penalidade de desempenho associado normalmente a criptografia e descriptografia de arquivos com grandes volumes de dados.

Em várias pesquisas realizadas pela Gartner, mais de 70% dos CIOs relataram que as questões de energia e resfriamento eram o maior desafio enfrentado no data center. Maximizar a eficiência energética pode ser particularmente importante para os clusters Hadoop, que crescem com o volume de dados.

Com seu sistema em um chip (SoC) design e envelopes de energia tão baixo quanto 6 watts, o processador Intel® Atom ™ oferece melhor densidade e energyefficiency para algumas cargas de trabalho. Como os processadores Intel Atom são baseados no conjunto de instruções x86 padrão da indústria, eles são compatíveis com todo o código de aplicativo que é executado em processadores Intel Xeon. Essa uniformidade ajuda a reduzir o custo total de propriedade, reduzindo o desenvolvimento de software, portando e custos de gerenciamento do sistema.

Além disso, a Intel oferece um software de gerenciamento de energia térmica chamado Intel® Data Center Manager (Intel® DCM). Intel DCM usa a instrumentação no Intel Xeon e os próximos processadores Intel Atom e integra-se com os consoles de gerenciamento existentes usando APIs padrão. Você pode usá-lo para monitorar os dados térmicos e de energia em tempo real para servidores e blades individuais e também para racks, linhas e grupos de servidores lógicos. Você pode limitar o uso de energia usando limites definidos ou políticas relacionadas à carga de trabalho, configurar alertas para eventos de energia e térmicos e armazenar e analisar dados para melhorar o planejamento de capacidade.

Memória

Memória de sistema suficiente é essencial para alta taxa de transferência de um grande número de tarefas paralelas do MapReduce. Hadoop normalmente requer de 48 GB a 96 GB de RAM por servidor e 64 GB é ótimo na maioria dos casos. Sempre equilibre a memória do servidor entre os canais de memória disponíveis para evitar gargalos de largura de banda de memória.

Os erros de memória são uma das causas mais comuns de falha de servidor e corrupção de dados, portanto a memória de código de correção de erros (ECC) é altamente recomendada.5 ECC é suportado em todos os servidores baseados em processadores Intel Xeon e em micro-servidores baseados no Intel Atom família de processadores.

Armazenamento

Cada servidor em um cluster Hadoop requer um número relativamente grande de unidades de armazenamento para evitar gargalos de E/S. Dois discos rígidos por núcleo de processador geralmente produzem bons resultados. No entanto, uma única unidade de SSD por núcleo pode oferecer maior taxa de transferência de E/S, latência reduzida e melhor desempenho global do cluster. Os SSDs SATA SSD da Série 710 da Intel® fornecem desempenho de leitura/gravação significativamente mais rápido do que os discos rígidos mecânicos, e esse desempenho extra pode ser valioso para aplicações latentes sensíveis ao MapReduce. Ele também pode acelerar trabalhos como arquivos de resultado intermediários são embaralhados entre as fases de map e reduce.

Os testes da Intel mostram que a substituição de unidades mecânicas por SSD Intel pode aumentar o desempenho em até 80%. Você também pode usar unidades mecânicas e SSDs juntos usando o Intel® Cache Acceleration Software. Esse modelo de armazenamento em camadas fornece alguns dos benefícios de desempenho dos SSDs com um custo de aquisição mais baixo.

Se você usar discos rígidos, drives SATA de 7.200 RPM oferecem um bom equilíbrio entre custo e desempenho. Execute as unidades no modo Advanced Host Controller Interface (AHCI) com Native Command Queuing (NCQ) habilitado para melhorar o desempenho para quando várias solicitações de leitura/gravação são invocadas simultaneamente. Embora você possa usar o RAID 0 para combinar logicamente unidades menores em um pool maior, o Hadoop orquestra automaticamente o aprovisionamento e a redundância de dados entre nós, pelo que o RAID não é recomendado.

Rede

Uma rede rápida não só permite que os dados sejam importados e exportados rapidamente, mas também pode melhorar o desempenho para a fase de em baralhamento de aplicações MapReduce.

Uma rede 10 Gigabit Ethernet fornece uma solução simples e econômica. Os testes da Intel demonstraram que o uso de 10 Gigabit Ethernet em vez de 1 Gigabit Ethernet em um cluster Hadoop pode melhorar o desempenho para operações chave em até 4x ao usar discos rígidos convencionais. A melhoria de desempenho é ainda maior quando se usa SSDs - até 6x. A melhoria com SSDs pode ser atribuída a gravações mais rápidas no subsistema de armazenamento.

À medida que um cluster do Hadoop cresce para incluir vários racks de servidores, você pode dimensionar o desempenho da rede conectando cada um dos switches de nível de rack 10 Gigabit Ethernet a um switch de nível de cluster de 40 Gigabit Ethernet. À medida que os requisitos continuam a aumentar, você pode interconectar vários switches de nível de cluster e adicionar uma ligação ascendente a uma infraestrutura de comutação de nível mais alto.
Programas

Apache Hadoop é um software de código aberto disponível gratuitamente no repositório de código-fonte apache.org. Um número de versões e implantações de teste podem ser baixadas diretamente para construção da sua plataforma com base na distribuição Apache Hadoop. Empresas e outras organizações que precisam de uma plataforma com suporte de fornecedor, no entanto, olham para um dos vários fornecedores independentes de software (ISVs) para fornecer um produto completo com software, atualizações e serviços.

O software Intel® Distribution para Apache Hadoop (Intel® Distribution) é uma plataforma de software empresarial que inclui o Apache Hadoop juntamente com outros componentes de software. A Distribuição da Intel tem vários recursos exclusivos.

• Construído a partir do silício para desempenho e segurança.
Executar a Intel Distribution em processadores Intel® permite que o Hadoop utilize plenamente os recursos de desempenho e segurança disponíveis no conjunto de instruções x86 em geral e no processador Intel Xeon em particular. Por exemplo, a Intel Distribution inclui aprimoramentos que aproveitam o Intel AES-NI, disponível nos processadores Intel Xeon, para acelerar as funções criptográficas, apagando a penalidade de desempenho típica de criptografia e descriptografia de arquivos em HDFS.

• Ajuste automático de desempenho.
Esse mecanismo integrado economiza tempo ao fornecer uma configuração mais otimizada. A Intel Distribution inclui um console de gerenciamento que fornece um mecanismo inteligente para configurar o cluster do Hadoop. O Intel Active Tuner usa um algoritmo genético para experimentar vários parâmetros e convergir rapidamente na configuração ideal para um determinado aplicativo Hadoop.

• Suporte para uma ampla gama de aplicações analíticas.
Esses aplicativos vêm de líderes no campo, a maioria dos quais já estão colaborando com a Intel na camada de hardware. Por exemplo, a Intel está trabalhando em conjunto com a Dell Kitenga, SAP, SAS, Revolution Analytics e vários outros ISVs analíticos para otimizar o desempenho ea escalabilidade da solução geral.

A Intel Distribution é vendida com suporte empresarial, treinamento e serviços profissionais a preços competitivos. É apoiado por uma equipe de software que tem experiência no mundo real com implantações Hadoop, bem como profunda experiência na pilha de plataforma completa do Apache Hadoop no hardware da Intel. Faça o download de uma versão de avaliação gratuita de 90 dias do software em http://hadoop.intel.com e entre em contato conosco para obter ajuda com implementações complexas de comprovação de conceito.

Conclusão

A mais nova onda de Big Data está gerando novas oportunidades e novos desafios para as empresas de todas as indústrias. O desafio da integração de dados corporativos, dados de mídias sociais e outros dados não estruturados em um ambiente de BI tradicional é um dos problemas mais urgentes enfrentados pelos CIOs e pelos gerentes de TI. O Apache Hadoop fornece uma plataforma econômica e amplamente escalável para gestão grandes volumes de dados e prepará-los para análise. Usar o Hadoop para manipular os processos ETL tradicionais pode reduzir o tempo de análise por horas ou mesmo dias. Executar o cluster Hadoop eficientemente significa selecionar uma infraestrutura ideal de servidores, armazenamento, rede e software. 

A Intel fornece software e componentes de plataforma de hardware para ajudá-lo a projetar e implantar um cluster Hadoop eficiente e de alto desempenho otimizado para grandes ETL de dados. Aproveite as arquiteturas de referência Intel, treinamento, serviços profissionais e suporte técnico para acelerar sua implantação e reduzir o risco.
